# ---------- Overriding hydra default configs ----------
hydra:
  job:
    name: happy_whale
  run:
    dir: ${out_dir}

# ---------- Default settings ----------
defaults:
  - dataset: happy_whale
  - optimizer: adam
  - scheduler: cosine_annealing

  # For hydra colorlog
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog


# ---------- Other configs ----------

#====
# Preprocessing
#====
preprocessing:
  h_resize_to: 1200
  w_resize_to: 1200
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]


#====
# Model
#====
model:
  restore_path: ${test_model}  # Restore model's weight from this path

  base_model: tf_efficientnet_b7  # Pretrained model to load
  backbone_class: null
  freeze_backbone: false

  head: ${head}
  output_dim: ${dataset.num_classes}
  output_dim_species: ${dataset.num_species_classes}
  pool: ${pool}
  use_bn: true
  embedding_size: -1
  in_chans: 3
  backbone2: false
  species_embedding_size: -1

pool:
  type: gem # adaptive or gem
  # gem
  p: 3
  p_trainable: false

#====
# Model head
#====
head:
  type: adaptive_arcface
  m: 0.2
  s: 20.0
  k: 3
  k_species: 3
  head_species: true
  s_species: 30
  use_penalty: true
  margin_coef_id: 0.27126
  margin_power_id: -0.364399
  margin_cons_id: 0.05
  margin_coef_species: 0.226253
  margin_power_species: -0.720133
  margin_cons_species: 0.05
  init: uniform
  init_species: uniform


#====
# Forwarder
#====
forwarder:
  max_epochs: ${training.epoch}
  head: ${head}
  backbone2: ${model.backbone2}
  species_embedding_size: ${model.species_embedding_size}

  tta:  # test-time augmentation
    flip_h: true
    rot90: false
    rot180: false


#====
# Dataset
#====
dataset:
  type: ???
  num_classes: ???


#====
# Data augmentation
# Should have effect only in training (not in validation nor test)
#====
# augmentation: null
augmentation:
  use_aug: true
  rotate: 15
  translate: 0.25
  shear: 3
  p_affine: 0.5
  crop_scale: 0.9
  crop_l: 0.75
  crop_r: 1.3333333333333333
  p_gray: 0.1
  p_blur: 0.05
  p_noise: 0.05
  p_downscale: 0.0
  p_shuffle: 0.3
  p_posterize: 0.2
  p_bright_contrast: 0.5
  p_cutout: 0.05
  p_snow: 0.0
  p_rain: 0.0
  p_sun: 0.0

#====
# Training
#====
training:
  project_name: happy_whale
  resume_from: null  # If set, restore all training state from that checkpoint
  debug: false  # If true, run in a debug mode
  use_wandb: false # If true, WandbLogger will be used
  seed: 0  # Random seed passed for seed_everything
  monitor: val/loss
  monitor_mode: min
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1

  epoch: 20
  batch_size: 16
  batch_size_test: 64
  num_gpus: 8
  num_workers: 16
  drop_last: true  # If true, drop the last incomplete batch in training phase
  use_amp: true  # If true, use 16-bit precision for training


#====
# Optimizer
#====
optimizer:
  type: ???
  use_sam: false  # If set, wrap optimizer with SAM
  lr_head: 1e-3
  lr_head_species: 1e-3


#====
# Scheduler
#====
scheduler:
  type: ???


#====
# Other essential configs
#====
out_dir: ???
test_model: null  # If set, only run test with that model
